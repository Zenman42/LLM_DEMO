"""LLM-based Entity Extractor — Pipeline Step 2a.

Uses gpt-4o-mini to extract all brand/company/product names from LLM response text,
then matches them against the target brand and competitor list using
case-insensitive + transliteration-aware fuzzy matching.

Falls back to empty list on failure (caller uses regex fallback).
"""

from __future__ import annotations

import json
import logging
import re

import httpx

from app.analysis.types import BrandMention, MentionType

logger = logging.getLogger(__name__)

# All Unicode dash/hyphen variants that LLMs commonly output.
# Normalized to ASCII hyphen-minus (U+002D) everywhere.
_DASH_RE = re.compile(r"[\u2010\u2011\u2012\u2013\u2014\u2015\u2212\u00AD\uFE58\uFE63\uFF0D]")

# ---------------------------------------------------------------------------
# LLM extraction prompt
# ---------------------------------------------------------------------------

_EXTRACTOR_SYSTEM_PROMPT = """\
You are a named entity extractor for AI visibility monitoring.

You receive a text generated by an AI assistant. Extract ALL named entities: \
brand names, company names, product names, service names, personal names \
(if they are public figures), and location names mentioned in the text.

For each entity, also capture the 1-2 sentences surrounding it for context, \
and classify the entity type.

Output ONLY valid JSON (no markdown, no explanation):
{"entities": [{"name": "...", "context": "...", "type": "..."}]}

Fields:
- name: entity exactly as it appears in the text
- context: 1-2 sentences from the text where the entity is mentioned (copy verbatim)
- type: one of "company", "product", "person", "location", "service", "other"

Rules:
- Include every named entity mentioned
- Preserve exact spelling from the text (Latin or Cyrillic)
- Include abbreviations and short forms (e.g. "BK" for Burger King)
- Do NOT include generic category names (e.g. "букмекер", "bookmaker", "app", "platform")
- If no entities found, return {"entities": []}"""

_EXTRACTOR_USER_TEMPLATE = """\
Extract all named entities with context from this AI response:

\"\"\"{text}\"\"\""""

# LLM config (same model as context_evaluator judge)
_MODEL = "gpt-4o-mini"
_API_URL = "https://api.openai.com/v1/chat/completions"
_MAX_TOKENS = 1024
_TEMPERATURE = 0.0
_TIMEOUT = 30
_MAX_TEXT_CHARS = 4000  # Truncate input to save tokens


# ---------------------------------------------------------------------------
# Enriched entity mention helpers
# ---------------------------------------------------------------------------


def normalize_entity_name(name: str) -> str:
    """Normalize Unicode dash variants to ASCII hyphen-minus in entity names.

    LLMs often output non-breaking hyphens (U+2011), en-dashes (U+2013), etc.
    that look identical to regular hyphens but cause exact-match failures in DB.
    """
    return _DASH_RE.sub("-", name.strip())


def get_entity_names(raw_entities: list) -> list[str]:
    """Extract plain entity names from raw_entities (supports both old and new format).

    Old format: ["name1", "name2"]
    New format: [{"name": "name1", "context": "...", "type": "..."}, ...]

    Returns list of strings (entity names) in both cases.
    Normalizes Unicode dashes to ASCII hyphen-minus.
    """
    if not raw_entities:
        return []
    first = raw_entities[0]
    if isinstance(first, str):
        return [normalize_entity_name(n) for n in raw_entities]
    if isinstance(first, dict):
        return [normalize_entity_name(e["name"]) for e in raw_entities if isinstance(e, dict) and "name" in e]
    return []


# ---------------------------------------------------------------------------
# Cyrillic ↔ Latin transliteration
# ---------------------------------------------------------------------------

_CYR_TO_LAT: dict[str, str] = {
    "а": "a",
    "б": "b",
    "в": "v",
    "г": "g",
    "д": "d",
    "е": "e",
    "ж": "zh",
    "з": "z",
    "и": "i",
    "й": "y",
    "к": "k",
    "л": "l",
    "м": "m",
    "н": "n",
    "о": "o",
    "п": "p",
    "р": "r",
    "с": "s",
    "т": "t",
    "у": "u",
    "ф": "f",
    "х": "kh",
    "ц": "ts",
    "ч": "ch",
    "ш": "sh",
    "щ": "shch",
    "ы": "y",
    "э": "e",
    "ю": "yu",
    "я": "ya",
    "ё": "yo",
    "ь": "",
    "ъ": "",
}

# Build reverse mapping (Latin → Cyrillic) for multi-char sequences first
_LAT_TO_CYR: dict[str, str] = {}
for _cyr, _lat in sorted(_CYR_TO_LAT.items(), key=lambda x: -len(x[1])):
    if _lat:  # Skip empty mappings (ь, ъ)
        _LAT_TO_CYR[_lat] = _cyr


def transliterate_to_latin(text: str) -> str:
    """Convert Cyrillic text to Latin transliteration."""
    result = []
    for ch in text.lower():
        result.append(_CYR_TO_LAT.get(ch, ch))
    return "".join(result)


def transliterate_to_cyrillic(text: str) -> str:
    """Convert Latin text to Cyrillic (best-effort reverse transliteration)."""
    result = []
    i = 0
    lower = text.lower()
    while i < len(lower):
        matched = False
        # Try longest multi-char sequences first (shch, sh, ch, zh, etc.)
        for length in (4, 3, 2, 1):
            chunk = lower[i : i + length]
            if chunk in _LAT_TO_CYR:
                result.append(_LAT_TO_CYR[chunk])
                i += length
                matched = True
                break
        if not matched:
            result.append(lower[i])
            i += 1
    return "".join(result)


# ---------------------------------------------------------------------------
# Phonetic skeleton matching for cross-script brand names
# ---------------------------------------------------------------------------
# Brand names often use non-standard transliteration for marketing:
#   "Winline" ↔ "Винлайн" (w↔в, i↔ай, ne↔н)
#   "Marathonbet" ↔ "Марафонбет" (th↔ф, on↔он)
# Standard GOST transliteration fails here, so we reduce both names to a
# "phonetic skeleton" — a consonant-based representation that normalizes
# common Latin↔Cyrillic phonetic substitutions.

# Map both Latin and Cyrillic chars to a shared phonetic consonant code
_PHONETIC_MAP: dict[str, str] = {
    # Labials: b, p, v, w, f, ph → grouped
    "б": "B",
    "b": "B",
    "п": "P",
    "p": "P",
    "в": "V",
    "v": "V",
    "w": "V",  # w ↔ в (Winline ↔ Винлайн)
    "ф": "F",
    "f": "F",
    # Dentals/alveolars
    "д": "D",
    "d": "D",
    "т": "T",
    "t": "T",
    "н": "N",
    "n": "N",
    "л": "L",
    "l": "L",
    "р": "R",
    "r": "R",
    "с": "S",
    "s": "S",
    "з": "Z",
    "z": "Z",
    # Velars/gutturals
    "к": "K",
    "k": "K",
    "c": "K",
    "q": "K",
    "г": "G",
    "g": "G",
    "х": "X",
    "h": "X",
    "x": "X",  # Latin x ≈ х in Russian brand names (1xBet↔1хБет)
    # Sibilants/affricates
    "ж": "J",
    "j": "J",
    "ч": "C",
    "ц": "C",
    "ш": "W",
    "щ": "W",
    # Vowels — map to a small set of groups to allow fuzzy vowel matching
    "а": "a",
    "a": "a",
    "о": "o",
    "o": "o",
    "у": "u",
    "u": "u",
    "е": "e",
    "e": "e",
    "э": "e",
    "ё": "e",
    "и": "i",
    "i": "i",
    "ы": "i",
    "y": "i",
    # Special
    "м": "M",
    "m": "M",
    "й": "",
    "ь": "",
    "ъ": "",
}

# Multi-char Latin sequences to normalize before single-char mapping
_LATIN_DIGRAPHS: list[tuple[str, str]] = [
    ("th", "T"),  # th → т (marathon → марафон: th≈ф, but skeleton: T)
    ("ph", "F"),  # ph → ф
    ("sh", "W"),  # sh → ш
    ("ch", "C"),  # ch → ч
    ("zh", "J"),  # zh → ж
    ("ck", "K"),  # ck → к
    ("gh", "G"),  # gh → г
    ("kh", "X"),  # kh → х
    ("ts", "C"),  # ts → ц
    ("ks", "KS"),  # explicit ks
]


def _phonetic_skeleton(text: str) -> str:
    """Reduce a name to a phonetic consonant skeleton.

    This strips vowels and normalizes Latin↔Cyrillic phonetic equivalences
    so that "Winline" and "Винлайн" produce similar skeletons.
    """
    s = text.lower().strip()
    # Remove non-alphanumeric (hyphens, dots, spaces)
    s = re.sub(r"[^a-zа-яёй0-9]", "", s)

    # Replace Latin digraphs first
    for digraph, code in _LATIN_DIGRAPHS:
        s = s.replace(digraph, code.lower())

    # Map each remaining char
    result = []
    for ch in s:
        mapped = _PHONETIC_MAP.get(ch, "")
        result.append(mapped)

    skeleton = "".join(result)
    # Remove vowels from the skeleton for consonant-only comparison
    consonant_only = re.sub(r"[aeiou]", "", skeleton)
    return consonant_only


def _phonetic_match(name_a: str, name_b: str) -> bool:
    """Check if two names match phonetically across scripts.

    Uses consonant skeleton comparison — good for catching brand-name
    transliterations like Winline↔Винлайн, Marathonbet↔Марафонбет.

    Guards against false positives:
    - Minimum skeleton length 4 for exact match, 5 for edit-distance match
    - Edit distance ≤ 1 only for skeletons of length ≥ 5
    - Names must use different scripts (Cyrillic vs Latin) for the match
      to apply — same-script names should be caught by simpler strategies
    """
    skel_a = _phonetic_skeleton(name_a)
    skel_b = _phonetic_skeleton(name_b)

    if not skel_a or not skel_b:
        return False

    # ── Script check: phonetic matching is designed for cross-script ──
    # transliterations (Cyrillic ↔ Latin). For same-script names,
    # other strategies (exact, containment, prefix) are more appropriate
    # and less error-prone.
    a_has_cyrillic = any("\u0400" <= ch <= "\u04ff" for ch in name_a)
    b_has_cyrillic = any("\u0400" <= ch <= "\u04ff" for ch in name_b)
    a_has_latin = any("a" <= ch.lower() <= "z" for ch in name_a)
    b_has_latin = any("a" <= ch.lower() <= "z" for ch in name_b)

    # If both names are in the same script, skip phonetic matching.
    # Exception: mixed-script names (e.g. "СберCloud") are allowed.
    a_is_pure_latin = a_has_latin and not a_has_cyrillic
    b_is_pure_latin = b_has_latin and not b_has_cyrillic
    a_is_pure_cyrillic = a_has_cyrillic and not a_has_latin
    b_is_pure_cyrillic = b_has_cyrillic and not b_has_latin

    if a_is_pure_latin and b_is_pure_latin:
        return False
    if a_is_pure_cyrillic and b_is_pure_cyrillic:
        return False

    # Require minimum skeleton length of 4 to avoid short-name false positives
    # (3 consonants is too few: RDS matches RedOS↔Redis, KLD matches Cloud4Y↔iCloud)
    if len(skel_a) < 4 or len(skel_b) < 4:
        return False

    # Exact skeleton match
    if skel_a == skel_b:
        return True

    # Allow one consonant difference for longer names (edit distance ≤ 1)
    # Require skeleton length ≥ 5 for fuzzy matching to reduce false positives
    # (4-char skeletons like MSKV↔MSKL produce too many spurious matches)
    if len(skel_a) >= 5 and len(skel_b) >= 5:
        if abs(len(skel_a) - len(skel_b)) <= 1:
            if _levenshtein_distance(skel_a, skel_b) <= 1:
                return True

    return False


def _levenshtein_distance(s1: str, s2: str) -> int:
    """Compute Levenshtein edit distance between two strings."""
    if len(s1) < len(s2):
        return _levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)

    prev_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        curr_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = prev_row[j + 1] + 1
            deletions = curr_row[j] + 1
            substitutions = prev_row[j] + (c1 != c2)
            curr_row.append(min(insertions, deletions, substitutions))
        prev_row = curr_row
    return prev_row[-1]


def generate_transliteration_aliases(name: str) -> list[str]:
    """Generate plausible transliteration variants of a brand name.

    For a Cyrillic name, generates Latin variants.
    For a Latin name, generates Cyrillic variants.
    Returns a list of aliases (may be empty if name has no cross-script potential).
    """
    aliases = set()
    has_cyrillic = bool(re.search(r"[а-яёА-ЯЁ]", name))
    has_latin = bool(re.search(r"[a-zA-Z]", name))

    if has_cyrillic and not has_latin:
        # Cyrillic → Latin: standard transliteration
        lat = transliterate_to_latin(name)
        if lat != name.lower():
            aliases.add(lat)
    elif has_latin and not has_cyrillic:
        # Latin → Cyrillic: reverse transliteration
        cyr = transliterate_to_cyrillic(name)
        if cyr != name.lower():
            aliases.add(cyr)

    return list(aliases)


# ---------------------------------------------------------------------------
# LLM entity extraction
# ---------------------------------------------------------------------------


async def extract_entities_with_llm(
    text: str,
    api_key: str,
    rpm_limiter=None,
    tpm_limiter=None,
) -> list[dict]:
    """Call gpt-4o-mini to extract named entities with context from text.

    Args:
        text: LLM response text (will be truncated to _MAX_TEXT_CHARS).
        api_key: OpenAI API key.
        rpm_limiter: Optional RpmLimiter to throttle OpenAI calls.
        tpm_limiter: Optional TpmLimiter to throttle OpenAI token usage.

    Returns:
        List of enriched entity dicts: [{"name": str, "context": str, "type": str}].
        Empty list on failure.
    """
    if rpm_limiter is not None:
        await rpm_limiter.acquire()
    if tpm_limiter is not None:
        # Estimate: ~1500 input + ~500 output (context included) = ~2000
        await tpm_limiter.acquire(tokens=2500)

    truncated = text[:_MAX_TEXT_CHARS]

    payload = {
        "model": _MODEL,
        "messages": [
            {"role": "system", "content": _EXTRACTOR_SYSTEM_PROMPT},
            {"role": "user", "content": _EXTRACTOR_USER_TEMPLATE.format(text=truncated)},
        ],
        "temperature": _TEMPERATURE,
        "max_tokens": _MAX_TOKENS,
    }

    try:
        async with httpx.AsyncClient(timeout=_TIMEOUT) as client:
            resp = await client.post(
                _API_URL,
                json=payload,
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                },
            )
            resp.raise_for_status()
            data = resp.json()

        raw = data["choices"][0]["message"]["content"] or ""

        # Try to extract JSON from markdown code blocks if present
        json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", raw, re.DOTALL)
        if json_match:
            raw = json_match.group(1)

        parsed = json.loads(raw)
        entities = parsed.get("entities", [])

        if not isinstance(entities, list):
            logger.warning("LLM extractor returned non-list entities: %s", type(entities))
            return []

        # Normalise: ensure each entity has the required fields
        result = []
        for i, e in enumerate(entities):
            if isinstance(e, str):
                # Old-format fallback: bare string without context
                if e.strip():
                    result.append({"name": normalize_entity_name(e), "context": "", "type": "other"})
            elif isinstance(e, dict) and "name" in e:
                result.append(
                    {
                        "name": normalize_entity_name(str(e["name"])),
                        "context": str(e.get("context", "")).strip(),
                        "type": str(e.get("type", "other")).strip().lower(),
                    }
                )

        logger.debug("LLM extracted %d entities: %s", len(result), [e["name"] for e in result[:10]])
        return result

    except httpx.TimeoutException:
        logger.warning("LLM entity extraction timed out after %ds", _TIMEOUT)
        return []
    except httpx.HTTPStatusError as e:
        logger.warning("LLM entity extraction HTTP error %s: %s", e.response.status_code, e)
        return []
    except (json.JSONDecodeError, KeyError, TypeError) as e:
        logger.warning("LLM entity extraction parse error: %s", e)
        return []
    except Exception as e:
        logger.warning("LLM entity extraction unexpected error: %s", e)
        return []


# ---------------------------------------------------------------------------
# Gemini-based entity extraction (fallback when OpenAI fails)
# ---------------------------------------------------------------------------

_GEMINI_MODEL = "gemini-2.0-flash"
_GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
_GEMINI_TIMEOUT = 30


async def extract_entities_with_gemini(
    text: str,
    api_key: str,
) -> list[str]:
    """Call Gemini Flash to extract brand/company names from text.

    Fallback extractor used when OpenAI entity extraction fails or returns empty.
    Uses the same prompt and parsing logic as the OpenAI version.

    Args:
        text: LLM response text (will be truncated to _MAX_TEXT_CHARS).
        api_key: Gemini API key.

    Returns:
        List of entity names as they appear in the text. Empty list on failure.
    """
    truncated = text[:_MAX_TEXT_CHARS]

    # Build the combined prompt (Gemini doesn't have a separate system message in
    # the basic generateContent API, so we merge system + user into one user turn)
    combined_prompt = _EXTRACTOR_SYSTEM_PROMPT + "\n\n" + _EXTRACTOR_USER_TEMPLATE.format(text=truncated)

    url = _GEMINI_API_URL.format(model=_GEMINI_MODEL)

    try:
        async with httpx.AsyncClient(
            timeout=httpx.Timeout(_GEMINI_TIMEOUT, connect=15.0),
        ) as client:
            resp = await client.post(
                url,
                params={"key": api_key},
                headers={"Content-Type": "application/json"},
                json={
                    "contents": [{"role": "user", "parts": [{"text": combined_prompt}]}],
                    "generationConfig": {
                        "temperature": 0.0,
                        "maxOutputTokens": 512,
                        "responseMimeType": "application/json",
                        "responseSchema": {
                            "type": "OBJECT",
                            "properties": {
                                "entities": {
                                    "type": "ARRAY",
                                    "items": {"type": "STRING"},
                                }
                            },
                            "required": ["entities"],
                        },
                    },
                },
            )
            if resp.status_code != 200:
                logger.warning(
                    "Gemini entity extraction HTTP error %d: %s",
                    resp.status_code,
                    resp.text[:300],
                )
                return []

        data = resp.json()
        candidates = data.get("candidates", [])
        if not candidates:
            logger.warning("Gemini entity extraction returned no candidates")
            return []

        parts = candidates[0].get("content", {}).get("parts", [])
        content = ""
        for part in parts:
            if "thought" not in part and "text" in part:
                content += part["text"]

        if not content:
            logger.warning("Gemini entity extraction returned empty content")
            return []

        # With responseMimeType=application/json, content should be valid JSON
        parsed = json.loads(content)
        entities = parsed.get("entities", [])

        if not isinstance(entities, list):
            logger.warning("Gemini extractor returned non-list entities: %s", type(entities))
            return []

        result = [normalize_entity_name(str(e)) for e in entities if e]
        logger.info("Gemini fallback extracted %d entities: %s", len(result), result[:10])
        return result

    except httpx.TimeoutException:
        logger.warning("Gemini entity extraction timed out after %ds", _GEMINI_TIMEOUT)
        return []
    except (json.JSONDecodeError, KeyError, TypeError) as e:
        logger.warning("Gemini entity extraction parse error: %s", e)
        return []
    except Exception as e:
        logger.warning("Gemini entity extraction unexpected error: %s", e)
        return []


# ---------------------------------------------------------------------------
# Fuzzy matching: extracted entities → BrandMention objects
# ---------------------------------------------------------------------------


def _normalize(name: str) -> str:
    """Normalize a name for comparison: lowercase, strip, collapse whitespace.

    Dots between words are replaced with spaces so "Яндекс.Карты" becomes
    "яндекс карты" for matching purposes.
    All Unicode dash variants (non-breaking hyphen, en-dash, em-dash, etc.)
    are replaced with a plain ASCII hyphen-minus (U+002D).
    """
    s = name.lower().strip()
    # Normalize all dash variants to ASCII hyphen-minus
    s = _DASH_RE.sub("-", s)
    # Replace dots between word chars with space (e.g. "Яндекс.Карты" → "яндекс карты")
    s = re.sub(r"(?<=\w)\.(?=\w)", " ", s)
    return re.sub(r"\s+", " ", s)


# Common word synonyms across languages used in brand names.
# Applied during matching so "Google Maps" ↔ "Google Карты".
_WORD_SYNONYMS: dict[str, str] = {
    "maps": "карты",
    "карты": "maps",
    "map": "карта",
    "карта": "map",
    "навигатор": "navigator",
    "navigator": "навигатор",
}


def _synonym_normalize(name: str) -> str:
    """Normalize name by replacing known synonym words with canonical forms.

    Returns lowercase, dots-expanded, synonyms-replaced string.
    """
    s = _normalize(name)
    words = s.split()
    canonical = []
    for w in words:
        # Replace synonym → canonical (always pick the Russian variant as canonical)
        syn = _WORD_SYNONYMS.get(w)
        if syn:
            canonical.append(min(w, syn))  # deterministic: pick alphabetically smaller
        else:
            canonical.append(w)
    return " ".join(canonical)


# Pre-compiled regexes for corporate prefix/suffix stripping
# Single corporate word (applied iteratively to handle chains like "Группа компаний")
_CORP_PREFIX_WORD = re.compile(
    r"^(?:группа|группы|гк|ск|ук|ао|оао|зао|пао|ооо|нпо|"
    r"компания|компаний|компании|корпорация|корпорации|концерн|концерна|"
    r"холдинг|холдинга|управляющая|строительная|девелоперская|"
    r"group|company|corp|corporation|holding|llc|inc)\s+",
    re.IGNORECASE,
)
_CORP_SUFFIXES = re.compile(
    r"\s+(?:group|inc|corp|llc|ltd|plc|gmbh|ag|co|"
    r"групп|группа|холдинг|девелопмент|development)$",
    re.IGNORECASE,
)


def _strip_corporate_prefix(name: str) -> str:
    """Strip common corporate prefixes/suffixes from a company name.

    Handles patterns like:
    - "Группа ПИК" → "ПИК"
    - "Группа компаний ПИК" → "ПИК"  (iterative stripping)
    - "ГК ФСК" → "ФСК"
    - "СК Донстрой" → "Донстрой"
    - "ООО Ромашка" → "Ромашка"
    - "Setl Group" → "Setl"
    - "Управляющая компания Самолёт" → "Самолёт"
    """
    s = name.strip()
    # Iteratively strip prefix words (handles chains like "Группа компаний")
    for _ in range(4):  # max 4 prefix words
        s_new = _CORP_PREFIX_WORD.sub("", s)
        if s_new == s:
            break
        s = s_new
    s = _CORP_SUFFIXES.sub("", s)
    return s.strip()


def _names_match(name_a: str, name_b: str) -> bool:
    """Check if two names match using multiple strategies.

    Strategies:
    1. Exact match (case-insensitive, dots normalized)
    2. Transliteration match (Cyrillic→Latin and Latin→Cyrillic)
    3. Phonetic skeleton match (cross-script: Winline ↔ Винлайн)
    4. Transliteration prefix (short brand form: PARI ↔ Париматч)
    5. Containment (one name is a substring of the other, ≥70% ratio)
    6. Synonym normalization ("Google Maps" == "Google Карты")
    7. Corporate prefix/suffix stripping ("Группа ПИК" == "ПИК")
    """
    a = _normalize(name_a)
    b = _normalize(name_b)

    # 1. Exact match
    if a == b:
        return True

    # 2. Transliteration match
    a_lat = transliterate_to_latin(name_a)
    b_lat = transliterate_to_latin(name_b)
    if _normalize(a_lat) == _normalize(b_lat):
        return True

    # Also try reverse: Latin → Cyrillic
    a_cyr = transliterate_to_cyrillic(name_a)
    b_cyr = transliterate_to_cyrillic(name_b)
    if _normalize(a_cyr) == _normalize(b_cyr):
        return True

    # 3. Phonetic skeleton match (handles non-standard transliterations
    #    like Winline↔Винлайн, Marathonbet↔Марафонбет)
    if _phonetic_match(name_a, name_b):
        return True

    # 4. Transliteration prefix match — handles short brand forms
    #    e.g. "PARI" ↔ "Париматч" (Pari is the short form of Parimatch)
    #    Require ≥4 chars and ≥40% coverage to avoid false positives
    if len(a) >= 4 and len(b) >= 4:
        a_lat_n = _normalize(a_lat)
        b_lat_n = _normalize(b_lat)
        shorter_l, longer_l = (a_lat_n, b_lat_n) if len(a_lat_n) <= len(b_lat_n) else (b_lat_n, a_lat_n)
        if len(shorter_l) >= 4 and longer_l.startswith(shorter_l) and len(shorter_l) / len(longer_l) >= 0.4:
            return True

    # 5. Containment (for cases like "BK" in "Burger King")
    #    Require that the shorter string covers at least 70% of the longer one
    #    to avoid false positives like "Фонбет" matching "Марафонбет"
    if len(a) >= 3 and len(b) >= 3:
        shorter, longer = (a, b) if len(a) <= len(b) else (b, a)
        if shorter in longer and len(shorter) / len(longer) >= 0.7:
            return True
        a_lat_n = _normalize(a_lat)
        b_lat_n = _normalize(b_lat)
        shorter_l, longer_l = (a_lat_n, b_lat_n) if len(a_lat_n) <= len(b_lat_n) else (b_lat_n, a_lat_n)
        if shorter_l in longer_l and len(shorter_l) / len(longer_l) >= 0.7:
            return True

    # 6. Synonym normalization
    #    "Google Maps" → "google maps" → synonym → "google карты"
    #    "Google Карты" → "google карты"
    #    Now they match!
    a_syn = _synonym_normalize(name_a)
    b_syn = _synonym_normalize(name_b)
    if a_syn == b_syn:
        return True

    # 7. Corporate prefix/suffix stripping
    #    "Группа ПИК" → "ПИК",  "ГК ФСК" → "ФСК",  "Setl Group" → "Setl"
    a_stripped = _normalize(_strip_corporate_prefix(name_a))
    b_stripped = _normalize(_strip_corporate_prefix(name_b))
    if a_stripped and b_stripped and a_stripped == b_stripped:
        return True
    # Also cross-check: stripped(a) vs original(b) and vice versa
    if a_stripped and a_stripped == b:
        return True
    if b_stripped and b_stripped == a:
        return True
    # Transliteration of stripped forms
    a_stripped_lat = _normalize(transliterate_to_latin(_strip_corporate_prefix(name_a)))
    b_stripped_lat = _normalize(transliterate_to_latin(_strip_corporate_prefix(name_b)))
    if a_stripped_lat and b_stripped_lat and a_stripped_lat == b_stripped_lat:
        return True

    return False


def _find_mention_in_text(text: str, entity_name: str) -> tuple[int, str]:
    """Find the entity in the original text and extract context.

    Returns:
        (char_offset, context_fragment). (-1, "") if not found.
    """
    # Try case-insensitive search for the exact entity name
    pattern = re.compile(re.escape(entity_name), re.IGNORECASE)
    match = pattern.search(text)
    if match:
        offset = match.start()
        # Use ±500 chars to capture enough context for the LLM judge,
        # especially for structured responses with pros/cons sections.
        ctx_start = max(0, offset - 500)
        ctx_end = min(len(text), offset + 500)
        context = text[ctx_start:ctx_end].strip()
        return offset, context

    return -1, ""


def _sentence_index(text: str, offset: int) -> int:
    """Determine which sentence (0-based) contains the given offset."""
    if offset < 0:
        return -1
    sentences = re.split(r"[.!?]\s+", text[:offset])
    return max(0, len(sentences) - 1)


def _brands_in_prompt(
    prompt_text: str,
    target_brand: str,
    target_aliases: list[str],
    competitors: dict[str, list[str]],
) -> set[str]:
    """Identify which known brands are already mentioned in the prompt.

    Returns a set of canonical brand names (target or competitor) found in
    the prompt text using the same fuzzy matching as entity resolution.
    Brands mentioned in the prompt should NOT count as organic brand mentions
    in the LLM response — the LLM is just echoing the question.
    """
    if not prompt_text:
        return set()

    found: set[str] = set()
    prompt_lower = prompt_text.lower()

    # Check target brand
    all_target_names = [target_brand] + (target_aliases or [])
    for tname in all_target_names:
        if not tname:
            continue
        if tname.lower() in prompt_lower:
            found.add(target_brand)
            break
        # Transliteration check
        if transliterate_to_latin(tname) in transliterate_to_latin(prompt_text):
            found.add(target_brand)
            break

    # Check competitors
    for comp_name, comp_aliases in (competitors or {}).items():
        all_comp_names = [comp_name] + (comp_aliases or [])
        for cname in all_comp_names:
            if not cname:
                continue
            if cname.lower() in prompt_lower:
                found.add(comp_name)
                break
            if transliterate_to_latin(cname) in transliterate_to_latin(prompt_text):
                found.add(comp_name)
                break

    return found


def match_entities(
    extracted: list[str],
    target_brand: str,
    target_aliases: list[str],
    competitors: dict[str, list[str]],
    full_text: str,
    prompt_text: str = "",
) -> tuple[BrandMention, list[BrandMention], list[str]]:
    """Match LLM-extracted entities against target brand and competitors.

    Args:
        extracted: Entity names extracted by the LLM.
        target_brand: Canonical brand name to track.
        target_aliases: Known aliases for the target brand.
        competitors: Dict of {competitor_name: [aliases]}.
        full_text: Full response text (for context extraction).
        prompt_text: Original prompt/query text. Brands already mentioned
            in the prompt are NOT counted as organic mentions in the response.

    Returns:
        (target_mention, competitor_mentions, discovered_entities)
        where discovered_entities are entities not matching any known brand.
    """
    # Determine which brands are already in the prompt (should not count as mentions)
    prompt_brands = _brands_in_prompt(
        prompt_text,
        target_brand,
        target_aliases,
        competitors,
    )
    if prompt_brands:
        logger.info(
            "Brands found in prompt (will not count as mentions): %s",
            prompt_brands,
        )

    target_mention = BrandMention(name=target_brand)
    comp_mentions: list[BrandMention] = []
    discovered: list[str] = []

    # Pre-build the list of all target names (canonical + aliases)
    target_names = [target_brand] + (target_aliases or [])

    # Pre-build competitor lookup: {canonical_name: [all_names]}
    comp_all_names: dict[str, list[str]] = {}
    for comp_name, comp_aliases in (competitors or {}).items():
        comp_all_names[comp_name] = [comp_name] + (comp_aliases or [])

    # Track which competitors have been matched
    matched_competitors: dict[str, str] = {}  # canonical_name → matched_entity

    # Skip target brand matching if it's already in the prompt
    target_in_prompt = target_brand in prompt_brands

    for entity in extracted:
        if not entity:
            continue

        # Try matching against target brand (only if NOT in prompt)
        if not target_in_prompt and not target_mention.is_mentioned:
            for tname in target_names:
                if _names_match(entity, tname):
                    offset, context = _find_mention_in_text(full_text, entity)
                    target_mention.is_mentioned = True
                    target_mention.aliases_matched = [entity]
                    target_mention.char_offset = offset
                    target_mention.sentence_index = _sentence_index(full_text, offset)
                    target_mention.mention_context = context
                    target_mention.mention_type = MentionType.DIRECT
                    break
            if target_mention.is_mentioned:
                continue

        # Try matching against competitors
        matched_comp = False
        for comp_name, all_names in comp_all_names.items():
            if comp_name in matched_competitors:
                continue
            # Skip competitor matching if it's already in the prompt
            if comp_name in prompt_brands:
                continue
            for cname in all_names:
                if _names_match(entity, cname):
                    matched_competitors[comp_name] = entity
                    matched_comp = True
                    break
            if matched_comp:
                break

        if not matched_comp:
            # Check entity doesn't match target brand or any alias
            matches_target = any(_names_match(entity, tn) for tn in target_names)
            if not matches_target:
                discovered.append(entity)

    # Build competitor BrandMention objects
    for comp_name in competitors or {}:
        mention = BrandMention(name=comp_name)
        if comp_name in matched_competitors:
            entity = matched_competitors[comp_name]
            offset, context = _find_mention_in_text(full_text, entity)
            mention.is_mentioned = True
            mention.aliases_matched = [entity]
            mention.char_offset = offset
            mention.sentence_index = _sentence_index(full_text, offset)
            mention.mention_context = context
            mention.mention_type = MentionType.DIRECT
        comp_mentions.append(mention)

    if discovered:
        logger.info(
            "Discovered %d entities not in known brands/competitors: %s",
            len(discovered),
            discovered[:10],
        )

    return target_mention, comp_mentions, discovered
